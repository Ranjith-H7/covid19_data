# -*- coding: utf-8 -*-
"""COVID19BDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1afKiNfuOMdOZokx6VPCS9GiNlCRsEe2X
"""

!pip install pyspark

from pyspark.sql import SparkSession
import os

# Initialize a Spark session
spark = SparkSession.builder \
    .appName("Read CSV with PySpark") \
    .getOrCreate()

# Path to the uploaded files
hospital_beds_file = "HospitalBedsIndia.csv"


statewise_testing_file = "StatewiseTestingDetails.csv"
covid_india_file = "covid_19_india.csv"
covid_vaccine_file = "covid_vaccine_statewise.csv"


# Function to check file existence and read CSV
def read_csv_file(file_path):
    if os.path.exists(file_path):
        return spark.read.csv(file_path, header=True, inferSchema=True)
    else:
        print(f"File not found: {file_path}")
        return None

# Read the CSV files
hospital_beds_df = read_csv_file(hospital_beds_file)


statewise_testing_df = read_csv_file(statewise_testing_file)
covid_india_df = read_csv_file(covid_india_file)
covid_vaccine_df = read_csv_file(covid_vaccine_file)

# reading the covid_19_india complete details
covid_india_df.printSchema()
#
covid_india_df.show(2)

# Create a temporary view
covid_india_df.createOrReplaceTempView("covid_india")

# Example SQL queries

# Query 1: Get the total number of confirmed cases
total_confirmed = spark.sql("SELECT SUM(Confirmed) as TotalConfirmed FROM covid_india")
total_confirmed.show()

# Query 2: Get the total number of deaths
total_deaths = spark.sql("SELECT SUM(Deaths) as TotalDeaths FROM covid_india")
total_deaths.show()

# Query 3: Get the total number of cured cases
total_cured = spark.sql("SELECT SUM(Cured) as TotalCured FROM covid_india")
total_cured.show()

# Query 4: Get the number of confirmed cases by State/UnionTerritory
confirmed_by_state = spark.sql("""
    SELECT `State/UnionTerritory`, SUM(Confirmed) as TotalConfirmed
    FROM covid_india
    GROUP BY `State/UnionTerritory`
    ORDER BY TotalConfirmed DESC
""")
confirmed_by_state.show()

# Query 5: Get the number of deaths by State/UnionTerritory
deaths_by_state = spark.sql("""
    SELECT `State/UnionTerritory`, SUM(Deaths) as TotalDeaths
    FROM covid_india
    GROUP BY `State/UnionTerritory`
    ORDER BY TotalDeaths DESC
""")
deaths_by_state.show()

import pandas as pd
import numpy as np
from pyspark.sql.functions import max as spark_max

# Assuming covid_india_df is already loaded with necessary data

# Group by State/UnionTerritory and get max values for Confirmed, Deaths, and Cured
state_cases = covid_india_df.groupBy('State/UnionTerritory').agg(
    spark_max('Confirmed').alias('Confirmed'),
    spark_max('Deaths').alias('Deaths'),
    spark_max('Cured').alias('Cured')
).toPandas()

# Calculate Active cases
state_cases['Active'] = state_cases['Confirmed'] - (state_cases['Deaths'] + state_cases['Cured'])

# Calculate Death Rate and Cure Rate
state_cases["Death Rate (per 100)"] = np.round(100 * state_cases["Deaths"] / state_cases["Confirmed"], 2)
state_cases["Cure Rate (per 100)"] = np.round(100 * state_cases["Cured"] / state_cases["Confirmed"], 2)

# Sort values by Confirmed cases and fill NaN values with 0
state_cases_sorted = state_cases.sort_values('Confirmed', ascending=False).fillna(0)

# Apply background gradients for visualization
styled_state_cases = state_cases_sorted.style.background_gradient(cmap='Reds', subset=["Confirmed"]) \
                                            .background_gradient(cmap='Blues', subset=["Deaths"]) \
                                            .background_gradient(cmap='Greens', subset=["Cured"]) \
                                            .background_gradient(cmap='Purples', subset=["Active"]) \
                                            .background_gradient(cmap='Greys', subset=["Death Rate (per 100)"]) \
                                            .background_gradient(cmap='Oranges', subset=["Cure Rate (per 100)"])

# Display the styled DataFrame
styled_state_cases

import IPython
IPython.display.HTML('<div class="flourish-embed flourish-bar-chart-race" data-src="visualisation/1977187" data-url="https://flo.uri.sh/visualisation/1977187/embed"><script src="https://public.flourish.studio/resources/embed.js"></script></div>')

# total beds reading india hospital beds
hospital_beds_df.printSchema()
hospital_beds_df.show(2)

#Create a Separate Column for State and Total Beds Available in Descending Order
from pyspark.sql.functions import col

# Calculate total beds (HMIS + NHP18) and add a column for state
total_beds_df = hospital_beds_df.withColumn("TotalBeds", col("NumPublicBeds_HMIS").cast("int") + col("NumRuralBeds_NHP18") + col("NumUrbanBeds_NHP18"))

# Selecting only State/UT and TotalBeds columns, sorted by TotalBeds in descending order
state_total_beds = total_beds_df.select("State/UT", "TotalBeds").orderBy(col("TotalBeds").desc())

state_total_beds.show(10)  # Displaying top 10 states by total beds available

# Create Columns for Different Categories of Facilities
# Adding columns for public beds + public facilities, district hospitals, rural, urban, primary + community health centers
facilities_df = hospital_beds_df.withColumn("PublicBeds_Facilities", col("NumPublicBeds_HMIS").cast("int") + col("TotalPublicHealthFacilities_HMIS").cast("int")) \
    .withColumn("DistrictHospitals", col("NumDistrictHospitals_HMIS").cast("int")) \
    .withColumn("RuralTotal", col("NumRuralHospitals_NHP18") * col("NumRuralBeds_NHP18")) \
    .withColumn("UrbanTotal", col("NumUrbanHospitals_NHP18") * col("NumUrbanBeds_NHP18")) \
    .withColumn("PrimaryCommunityHealth", col("NumPrimaryHealthCenters_HMIS").cast("int") + col("NumCommunityHealthCenters_HMIS").cast("int"))

facilities_df.select("State/UT", "PublicBeds_Facilities", "DistrictHospitals", "RuralTotal", "UrbanTotal", "PrimaryCommunityHealth").show(20)

from pyspark.sql.functions import col

# Calculate total beds (HMIS + NHP18) and add a column for state
total_beds_df = hospital_beds_df.withColumn(
    "TotalBeds",
    col("NumPublicBeds_HMIS").cast("int") + col("NumRuralBeds_NHP18") + col("NumUrbanBeds_NHP18")
)

# Selecting only State/UT and TotalBeds columns, sorted by TotalBeds in descending order
state_total_beds = total_beds_df.select("State/UT", "TotalBeds").orderBy(col("TotalBeds").desc())

# Convert to Pandas DataFrame for plotting
state_total_beds_pd_df = state_total_beds.toPandas()

# Drop rows with None values (if any)
state_total_beds_pd_df.dropna(inplace=True)

# Plotting the bar chart
plt.figure(figsize=(14, 8))
plt.bar(state_total_beds_pd_df["State/UT"], state_total_beds_pd_df["TotalBeds"], color='blue')
plt.xlabel('State/UT')
plt.ylabel('Total Beds')
plt.title('State-wise Total Beds in Healthcare Facilities')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

#read the statewise testing details
statewise_testing_df.printSchema()
statewise_testing_df.show(2)

from pyspark.sql.functions import col, when, year, month, dayofmonth
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

# Convert 'Negative' column to numeric and handle missing values
statewise_testing_df = statewise_testing_df.withColumn('Negative', col('Negative').cast('double'))
statewise_testing_df = statewise_testing_df.withColumn('Negative', when(col('Negative').isNull(), 0).otherwise(col('Negative')))

# Handle missing values in the 'Positive' column
statewise_testing_df = statewise_testing_df.withColumn('Positive', col('Positive').cast('double'))
statewise_testing_df = statewise_testing_df.withColumn('Positive', when(col('Positive').isNull(), 0).otherwise(col('Positive')))

# Extract year, month, day from 'Date' column
statewise_testing_df = statewise_testing_df.withColumn('Year', year(col('Date')))
statewise_testing_df = statewise_testing_df.withColumn('Month', month(col('Date')))
statewise_testing_df = statewise_testing_df.withColumn('Day', dayofmonth(col('Date')))

# String indexer for 'State' column
state_indexer = StringIndexer(inputCol='State', outputCol='StateIndex')

# Define features and assembler
features = ['Year', 'Month', 'Day', 'StateIndex', 'Negative']
assembler = VectorAssembler(inputCols=features, outputCol='features')

# Initialize regression model
lr = LinearRegression(featuresCol='features', labelCol='Positive')

# Pipeline for assembling features and applying regression
pipeline = Pipeline(stages=[state_indexer, assembler, lr])

# Split data into training and testing sets
train_data, test_data = statewise_testing_df.randomSplit([0.8, 0.2], seed=123)

# Train the model
model = pipeline.fit(train_data)

# Predictions on test data
predictions = model.transform(test_data)

# Evaluate the model
evaluator = RegressionEvaluator(labelCol='Positive', predictionCol='prediction', metricName='rmse')
rmse = evaluator.evaluate(predictions)
r2 = evaluator.evaluate(predictions, {evaluator.metricName: 'r2'})

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")

# Ensure statewise_testing_df is properly loaded as a Spark DataFrame
statewise_testing_df = spark.read.format("csv").option("header", "true").load(statewise_testing_file)

# Register the DataFrame as a temporary view to use with Spark SQL
statewise_testing_df.createOrReplaceTempView("statewise_testing")

# Perform SQL query to aggregate Positive and Negative cases by State
query = """
    SELECT State, SUM(Positive) AS Total_Positive, SUM(Negative) AS Total_Negative
    FROM statewise_testing
    GROUP BY State
    ORDER BY State
"""

# Execute SQL query
statewise_cases = spark.sql(query)

# Convert Spark DataFrame to Pandas for plotting
statewise_cases_pd = statewise_cases.toPandas()

# Plotting the data using Matplotlib
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
plt.barh(statewise_cases_pd['State'], statewise_cases_pd['Total_Negative'], label='Negative', color='green')
plt.barh(statewise_cases_pd['State'], statewise_cases_pd['Total_Positive'], left=statewise_cases_pd['Total_Negative'], label='Positive', color='red')

# Adding labels and title
plt.xlabel('Number of Cases')
plt.ylabel('State')
plt.title('COVID-19 Positive vs. Negative Cases by State')
plt.legend()
plt.tight_layout()
plt.show()

# SQL query to compute positive rate (Positive / TotalSamples) by State
query_positive_rate = """
    SELECT State,
           SUM(Positive) AS Total_Positive,
           SUM(TotalSamples) AS Total_Samples,
           SUM(Positive) / SUM(TotalSamples) AS Positive_Rate
    FROM statewise_testing
    GROUP BY State
    ORDER BY State
"""

# Execute SQL query
positive_rate_by_state = spark.sql(query_positive_rate)
positive_rate_by_state.show()

# SQL query to filter rows based on a date range
query_date_range = """
    SELECT *
    FROM statewise_testing
    WHERE Date >= '2020-04-01' AND Date <= '2020-04-30'
"""

# Execute SQL query
filtered_data = spark.sql(query_date_range)
filtered_data.show()

covid_vaccine_df.printSchema();
covid_vaccine_df.show(2)

#Show the total number of individuals vaccinated by age group for each state
query5 = spark.sql("""
    SELECT State,
           SUM(`18-44 Years(Individuals Vaccinated)`) as Vaccinated_18_44,
           SUM(`45-60 Years(Individuals Vaccinated)`) as Vaccinated_45_60,
           SUM(`60+ Years(Individuals Vaccinated)`) as Vaccinated_60_plus
    FROM covid_vaccine
    GROUP BY State
    ORDER BY State
""")
query5.show()

query6 = spark.sql("""
    SELECT State,
           SUM(`Male(Individuals Vaccinated)`) as Male_Vaccinated,
           SUM(`Female(Individuals Vaccinated)`) as Female_Vaccinated,
           SUM(`Transgender(Individuals Vaccinated)`) as Transgender_Vaccinated
    FROM covid_vaccine
    GROUP BY State
    ORDER BY State
""")

query6_data = query6.toPandas()
import matplotlib.pyplot as plt

# Sum the data for the pie chart
total_vaccinated = query6_data[['Male_Vaccinated', 'Female_Vaccinated', 'Transgender_Vaccinated']].sum()

# Data for the pie chart
labels = ['Male', 'Female', 'Transgender']
sizes = [total_vaccinated['Male_Vaccinated'], total_vaccinated['Female_Vaccinated'], total_vaccinated['Transgender_Vaccinated']]

# Create the pie chart
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.title('Distribution of Individuals Vaccinated by Gender')
plt.show()

covid_vaccine_df.createOrReplaceTempView("covid_vaccine")

query1 = spark.sql("""
    SELECT State, SUM(`Total Doses Administered`) as Total_Doses_Administered
    FROM covid_vaccine
    GROUP BY State
    ORDER BY Total_Doses_Administered DESC
""")

query1_data = query1.toPandas()
covid_vaccine_df.createOrReplaceTempView("covid_vaccine")

import matplotlib.pyplot as plt

# Plotting the data
plt.figure(figsize=(12, 8))
plt.bar(query1_data['State'], query1_data['Total_Doses_Administered'], color='blue')

# Adding labels and title
plt.xlabel('State')
plt.ylabel('Total Doses Administered')
plt.title('Total COVID-19 Vaccine Doses Administered by State')

# Rotating x-axis labels for better visibility
plt.xticks(rotation=90)

# Display the plot
plt.tight_layout()
plt.show()